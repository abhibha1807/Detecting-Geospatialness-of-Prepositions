{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x16FwULqk1wp"
      },
      "source": [
        "# Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrjaEkHDVzZv"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re \n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unyrQQL7k-f0"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPrytRgzrquz"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "rows=[]\n",
        "filename=('newbig.csv')\n",
        "with open(filename, 'r') as csvfile: \n",
        "    data = csv.reader(csvfile) \n",
        "    for row in data:\n",
        "        rows.append(row)\n",
        "    rows.pop(0)\n",
        "print(len(rows))\n",
        "\n",
        "print(len(rows))\n",
        "for i in range(10):\n",
        "    print(rows[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "le78L-1IryUN"
      },
      "outputs": [],
      "source": [
        "#preprocesses data by extracting the window of words around the target preposition. We use stemmers and lemmatisers for preprocessing the text.\n",
        "def remove(list): \n",
        "    pattern = '[0-9]'\n",
        "    list = [re.sub(pattern, '', i) for i in list] \n",
        "    return list\n",
        "temp=[]\n",
        "sentence_list=[]\n",
        "Y=[]\n",
        "X=[]\n",
        "    \n",
        "n=len(rows)\n",
        "for i in range(n):\n",
        "    print(rows[i])\n",
        "    words=[]\n",
        "    l=[]\n",
        "    l=rows[i][0].split(' ')\n",
        "    print(l)\n",
        "    l=remove(l)\n",
        "    if rows[i][1] == 'p70' or rows[i][1] == '9' or rows[i][1] == 'MASCOT':\n",
        "        continue\n",
        "    Y.append(rows[i][2])\n",
        "    k=l.index(rows[i][1])   \n",
        "    l_limit=k-5\n",
        "    u_limit=k+5\n",
        "    if l_limit<=0:\n",
        "        l_limit=0\n",
        "\n",
        "    if u_limit>len(l):\n",
        "        u_limit=len(l)\n",
        "\n",
        "    words=l[l_limit:u_limit+1]\n",
        "    words.remove(l[k])\n",
        "    sw = stopwords.words('english')\n",
        "    for i in words:\n",
        "        for j in sw:\n",
        "            if i == j:\n",
        "                words.remove(i)\n",
        "    new_words=[]\n",
        "    for i in words:\n",
        "        stemmer=SnowballStemmer('english')\n",
        "        k=stemmer.stem(i)\n",
        "        new_words.append(k)\n",
        "    final_string=''\n",
        "    for i in new_words:\n",
        "        final_string=final_string+' '+i\n",
        "   \n",
        "       sentence_list.append(final_string)\n",
        "\n",
        "print(len(sentence_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5bAraivjc-i"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "matrix=TfidfVectorizer(max_features=3000)\n",
        "X = matrix.fit_transform(sentence_list).toarray()\n",
        "\n",
        "X_data=np.array(X)\n",
        "Y_data=np.array(Y)\n",
        "\n",
        "print(type(X_data))\n",
        "print(type(Y_data))\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iHi5K01kp9W"
      },
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6bq6OEajkU2"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict Class\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Accuracy \n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mgryJeGlJF_"
      },
      "source": [
        "## Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2Lgwh27lYfl"
      },
      "source": [
        "hyper parameter tuning was first done using randomised search CV and then we used grid search CV to get the o[timal set of hyper parameters by considering values close to the random value predicted by the randomised search CV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faHcaP-vlMm8"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Number of trees in random forest\n",
        "# n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "n_estimators = [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt', 10, 50, 100]\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10, 20]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 5, 10]\n",
        "# Method of selecting samples for training each tree\n",
        "# bootstrap = [True, False]\n",
        "bootstrap = [True]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gjoTFXXlRjU"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of random search \n",
        "param_grid = {\n",
        "    'bootstrap': [True],\n",
        "    'max_depth': [90,100,150],\n",
        "    'max_features': [40,50,60],\n",
        "    'min_samples_leaf': [1],\n",
        "    'min_samples_split': [4,5,6],\n",
        "    'n_estimators': [300,400,500]\n",
        "}\n",
        "# Create a based model\n",
        "rf = RandomForestClassifier()\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
        "                          cv = 3, n_jobs = -1, verbose = 2)\n",
        "\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6by37ARFlnU1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "rfc = RandomForestClassifier(bootstrap =True,max_depth=90,max_features=50,min_samples_leaf= 1,min_samples_split=5,n_estimators=500)\n",
        "rfc.fit(X_train,y_train)\n",
        "y_pred = rfc.predict(X_test)\n",
        "scores = cross_val_score(rfc, X_train, y_train, cv=5)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xGuPo74mFhH"
      },
      "source": [
        "## SVM "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hD3XQ8srmUE3"
      },
      "source": [
        "Since literature suggests to use Grid Search CV for finding the optimal set of hyperparameters for SVM's we have considered a few values and then applied this technique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wob3WXvMmIt4"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "def svc_param_selection(X, y, nfolds):\n",
        "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
        "    gammas = [0.001, 0.01, 0.1, 1]\n",
        "    kernels=[ 'rbf','linear',]\n",
        "    param_grid = {'C': Cs, 'kernel': kernels,'gamma' : gammas}\n",
        "    grid_search = GridSearchCV(svm.SVC(), param_grid, cv=nfolds)\n",
        "    grid_search.fit(X, y)\n",
        "    print(grid_search.best_params_)\n",
        "\n",
        "svc_param_selection(X_train, y_train,5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVvASfHLm045"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import KFold\n",
        "clf = svm.SVC(kernel='linear', gamma=0.01, C=10) \n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_3OE5G9nHNi"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4PBllYOkvdT"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkVDogHqjr5S"
      },
      "outputs": [],
      "source": [
        "conf_mat = [[0,0],[0,0]]\n",
        "y_pred=y_pred.tolist()\n",
        "y_test=y_test.tolist()\n",
        "\n",
        "i=0\n",
        "print(\"confusion matrix:\")\n",
        "\n",
        "while i < len(y_pred):\n",
        "    \n",
        "    if y_pred[i]=='1' and y_test[i]=='1': \n",
        "        conf_mat[1][1] +=1\n",
        "    elif y_pred[i]=='0' and  y_test[i]=='1': \n",
        "        conf_mat[1][0] +=1\n",
        "    elif y_pred[i]=='1' and  y_test[i]=='0': \n",
        "        conf_mat[0][1] +=1\n",
        "    elif y_pred[i]=='0' and y_test[i]== '0': \n",
        "        conf_mat[0][0] +=1\n",
        "    i=i+1\n",
        "print(conf_mat)\n",
        "for list1 in conf_mat:\n",
        "\tprint (list1)\n",
        "\n",
        "print(\"\\n\")\t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "pH0vfT2Uj3Nm",
        "outputId": "18824e20-84f8-4a41-d4ec-175653e1783b"
      },
      "outputs": [],
      "source": [
        "\n",
        "if conf_mat[1][1]==0 and conf_mat[1][0]==0:\n",
        "\tp = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[0][1])+1)\n",
        "else:\n",
        "\tp = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[0][1]))\n",
        "\n",
        "if conf_mat[1][1]==0 and conf_mat[0][1]==0:\n",
        "\tr = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[1][0]+1))\n",
        "else:\n",
        "\tr = (float(conf_mat[1][1]))/(float(conf_mat[1][1]) + float(conf_mat[1][0]))\n",
        "\n",
        "\n",
        "acc= (float(conf_mat[0][0])+float(conf_mat[1][1]))/(float(conf_mat[0][0])+float(conf_mat[0][1])+float(conf_mat[1][0])+float(conf_mat[1][1]))\n",
        "\n",
        "\n",
        "if p==0 and r==0:\n",
        "\tF1=2*p*r/(p+r+1)\n",
        "else :\n",
        "\tF1=2*p*r/(p+r)\n",
        " \n",
        "print('precision:', p)\n",
        "print('recall:', r)\n",
        "print('F1:', F1)\n",
        "print('accuracy:', acc)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Naive Bayes Classifier.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
